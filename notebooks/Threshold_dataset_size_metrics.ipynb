{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Thresholds and Their Impact on Precision and Recall\n",
        "\n",
        "This notebook is a hands-on lab for exploring how classifiers behave on **imbalanced datasets**. It uses a single 2D synthetic dataset that can be re-generated with different parameters, such as **class imbalance, sample size, and noise**.\n",
        "\n",
        "In this exercise, you will learn about:\n",
        "\n",
        "* **Thresholds** and how they affect predicted class labels.\n",
        "* **Probabilistic classification** — how models output probabilities instead of hard labels.\n",
        "* **Sample size effects** — how small vs large training sets influence performance metrics.\n",
        "* **Threshold-independent metrics** — ROC AUC and Average Precision (PR AUC), which summarize model performance without committing to a single threshold.\n",
        "\n",
        "> Understanding these concepts is essential for choosing the right metric and threshold in real-world, imbalanced classification problems."
      ],
      "metadata": {
        "id": "ckAaLg_J2LJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 0. Setup / Imports\n",
        "\n"
      ],
      "metadata": {
        "id": "qUG1Kk9D3FGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, accuracy_score, precision_score,\n",
        "    recall_score, f1_score, roc_curve, roc_auc_score, precision_recall_curve,\n",
        "    average_precision_score\n",
        ")\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.utils import resample\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "6RJ55nFT3Jcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 1. Helper functions\n",
        "We’ll create two helpful utilities:\n",
        "- `generate_data()` to produce the synthetic 2D dataset with controllable parameters.\n",
        "- `plot_decision_boundary()` to show the model decision regions; it supports passing a\n",
        "  probability threshold to visualize thresholded decision boundaries for probability-based models.\n"
      ],
      "metadata": {
        "id": "P4JEkrKL3Lh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(\n",
        "    n_samples=2000,\n",
        "    weights=(0.95, 0.05),\n",
        "    class_sep=1.0,\n",
        "    flip_y=0.01,\n",
        "    random_state=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate a 2D classification dataset with controllable imbalance and overlap.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_samples : int\n",
        "    weights : tuple (neg_weight, pos_weight)\n",
        "    class_sep : float, larger => easier to separate\n",
        "    flip_y : float, label noise\n",
        "    random_state : int or None\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X : ndarray (n_samples, 2)\n",
        "    y : ndarray (n_samples,)\n",
        "    \"\"\"\n",
        "    X, y = make_classification(\n",
        "        n_samples=n_samples,\n",
        "        n_features=2,\n",
        "        n_redundant=0,\n",
        "        n_clusters_per_class=1,\n",
        "        weights=list(weights),\n",
        "        class_sep=class_sep,\n",
        "        flip_y=flip_y,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# Decision boundary plotting utility\n",
        "from matplotlib import cm\n",
        "\n",
        "\n",
        "def plot_decision_boundary(model, X, y, ax=None, threshold=0.5, proba=True, title=None, mesh_step=0.02):\n",
        "    \"\"\"\n",
        "    Plot a 2D decision boundary for a classifier.\n",
        "\n",
        "    If model supports `predict_proba` or `decision_function` this function can visualize\n",
        "    thresholded decision regions. If not (e.g. KNeighbors or decision tree without probabilities),\n",
        "    set `proba=False` and it will use `model.predict`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : fitted classifier\n",
        "    X, y : data\n",
        "    ax : matplotlib axis\n",
        "    threshold : float in [0,1] (used only if proba=True)\n",
        "    proba : bool - whether to use probabilities/scores\n",
        "    title : str\n",
        "    \"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(6,5))\n",
        "    else:\n",
        "        fig = ax.figure\n",
        "\n",
        "    x_min, x_max = X[:, 0].min() - 1.0, X[:, 0].max() + 1.0\n",
        "    y_min, y_max = X[:, 1].min() - 1.0, X[:, 1].max() + 1.0\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, mesh_step), np.arange(y_min, y_max, mesh_step))\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "    try:\n",
        "        if proba and hasattr(model, 'predict_proba'):\n",
        "            Z = model.predict_proba(grid)[:, 1]\n",
        "            Zf = (Z >= threshold).astype(int)\n",
        "            cmap = cm.RdYlBu\n",
        "            Zplot = Z.reshape(xx.shape)\n",
        "            cont = ax.contourf(xx, yy, Zplot, levels=20, alpha=0.4, cmap=cmap)\n",
        "            ax.contour(xx, yy, Z.reshape(xx.shape), levels=[threshold], colors=['k'], linewidths=1.2)\n",
        "        elif proba and hasattr(model, 'decision_function'):\n",
        "            scores = model.decision_function(grid)\n",
        "            # scale scores to 0..1 using min-max for visualization (not probabilities)\n",
        "            scores_scaled = (scores - scores.min()) / (scores.max() - scores.min() + 1e-12)\n",
        "            ax.contourf(xx, yy, scores_scaled.reshape(xx.shape), levels=20, alpha=0.4)\n",
        "            ax.contour(xx, yy, scores_scaled.reshape(xx.shape), levels=[threshold], colors=['k'], linewidths=1.2)\n",
        "        else:\n",
        "            Z = model.predict(grid)\n",
        "            Zf = Z.reshape(xx.shape)\n",
        "            ax.contourf(xx, yy, Zf, alpha=0.3, cmap=cm.coolwarm)\n",
        "    except Exception as e:\n",
        "        # fallback\n",
        "        try:\n",
        "            Z = model.predict(grid)\n",
        "            ax.contourf(xx, yy, Z.reshape(xx.shape), alpha=0.3, cmap=cm.coolwarm)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # scatter\n",
        "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k', cmap=cm.coolwarm)\n",
        "    ax.set_xlim(xx.min(), xx.max())\n",
        "    ax.set_ylim(yy.min(), yy.max())\n",
        "    ax.set_xlabel('x1')\n",
        "    ax.set_ylabel('x2')\n",
        "    if title is not None:\n",
        "        ax.set_title(title)\n",
        "    return ax"
      ],
      "metadata": {
        "id": "kODXbIN-3Nsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 2. Generate a base dataset and visualize\n",
        "We’ll create a default unified dataset: 2000 samples, 95% negative, 5% positive, moderate separation.\n",
        "\n"
      ],
      "metadata": {
        "id": "WnhTZwZw3Oqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_sep= 1    # less separation 1 is easy, o → more overlap\n",
        "flip_y=0.01     #  noisy labels\n",
        "\n",
        "\n",
        "X, y = generate_data(n_samples=2000, weights=(0.95, 0.05), class_sep= class_sep, flip_y= flip_y, random_state=0)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.scatter(X[:,0], X[:,1], c=y, s=15, cmap='coolwarm', edgecolors='k')\n",
        "plt.title('Base dataset: 2D synthetic (95/5 imbalance)')\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "btOoOHjf3SLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show class counts\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "print(f\"Class 0 count: {counts[0]} | Class 1 count: {counts[1]}\")\n"
      ],
      "metadata": {
        "id": "1iR9l3Ug6NCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train the models\n",
        "We’ll use **KNN**, **Decision Tree**, **SVM (with probability enabled)**, and **Logistic Regression**.\n",
        "\n",
        "*Short explanation of Logistic Regression:*  \n",
        "It is a linear model that predicts the log-odds of the positive class and produces probabilities via the logistic (sigmoid) function.\n",
        "\n",
        "⚠️ **Note on hyper-parameters:**  \n",
        "The ones used here are **not** optimal — they’re chosen for clarity. Feel free to experiment by changing them later.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ngwHxUKi3TRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=1)\n",
        "\n",
        "# Standardize for distance-based models (KNN, SVM) — keep pipeline simple here\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "models = {\n",
        "    'knn': KNeighborsClassifier(n_neighbors=5),\n",
        "    'tree': DecisionTreeClassifier(max_depth=5, random_state=1),\n",
        "    'svm': SVC(probability=True, gamma='scale', C=1.0, random_state=1),\n",
        "    'logreg': LogisticRegression(solver='lbfgs', max_iter=1000, random_state=1)\n",
        "}\n",
        "\n",
        "fitted = {}\n",
        "for name, model in models.items():\n",
        "    if name in ('knn', 'svm'):\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "    fitted[name] = model"
      ],
      "metadata": {
        "id": "z3mN9JSQ3Va2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔑 Key Concept: Probabilities and Thresholds\n",
        "\n",
        "Many classifiers can output **probabilities** (or probability-like scores) for each class, not just hard labels.\n",
        "By default, most models assign the positive class if the probability ≥ **0.5** (the default threshold).\n",
        "\n",
        "Thresholds can be adjusted:\n",
        "\n",
        "* **Lower threshold (e.g., 0.3):** more positives → higher recall, but more false positives.\n",
        "* **Higher threshold (e.g., 0.7):** fewer positives → higher precision, but more false negatives.\n",
        "\n",
        "**Why it matters:**\n",
        "Threshold choice is central to this lab. Metrics like F1, precision, and recall change as the threshold changes. The “best” threshold depends on *context* or *business value*, not just math.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 First look: probabilities and thresholds\n",
        "\n",
        "Let’s inspect model outputs. Instead of `predict()`, which returns 0/1, many models provide **predicted probabilities** with `predict_proba()`.\n",
        "\n",
        "**Example output of the code below:**\n",
        "\n",
        "```\n",
        "Sample 0: True=0, Prob=0.00, Pred@0.5=0\n",
        "```\n",
        "\n",
        "**How to read it:**\n",
        "\n",
        "* **Sample 0:** index of the test sample.\n",
        "* **True=0:** actual class from the test set (`0` = negative).\n",
        "* **Prob=0.00:** model’s predicted probability of being positive. Close to 0 means very confident it’s negative.\n",
        "* **Pred@0.5=0:** predicted label using threshold 0.5. Since Prob < 0.5, the model predicts class 0.\n",
        "\n",
        "**Key takeaways:**\n",
        "\n",
        "* Probabilities show the model’s confidence, not just a label.\n",
        "* The predicted label depends on the threshold; adjusting it can flip borderline predictions.\n",
        "* Understanding this explains why **precision, recall, and F1** vary with threshold.\n",
        "\n"
      ],
      "metadata": {
        "id": "2SHyu1u_73g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Pick 3 sample points from the test set\n",
        "X_sample = X_test_scaled[:3]\n",
        "y_sample = y_test[:3]\n",
        "\n",
        "for name, model in fitted.items():\n",
        "    if name in ('knn', 'svm'):\n",
        "        probs = model.predict_proba(X_sample)[:, 1]\n",
        "    else:\n",
        "        probs = model.predict_proba(X_test[:3])[:, 1]\n",
        "\n",
        "    print(f\"\\n{name.upper()} predictions (3 sample points):\")\n",
        "    for i, (true_label, prob) in enumerate(zip(y_sample, probs)):\n",
        "        # simple threshold demonstration\n",
        "        threshold= 0.5  # between 0 and 1\n",
        "        pred_default = int(prob >= threshold) # is the probability higher or lower than the threshild\n",
        "        print(f\"Sample {i}: True={true_label}, Prob={prob:.2f}, Pred@{threshold}={pred_default}\")\n"
      ],
      "metadata": {
        "id": "IHmMkO46769G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code above change the threshold of 0.5 and re-run\n",
        "```\n",
        "        pred_default = int(prob >= 0.5)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xHC7MTw9pWQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize decision boundaries (default threshold 0.5) for each model. For KNN and tree the `predict_proba` exists for KNN and tree; for tree we used unscaled X; for SVM & KNN we visualized using scaled features but plotted on original X space — this is an approximation (the mesh uses original X coordinates). This is intentionally kept simple for didactic purposes.\n"
      ],
      "metadata": {
        "id": "iUBmxhbh62fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(12,10))\n",
        "axes = axes.ravel()\n",
        "for ax, (name, model) in zip(axes, fitted.items()):\n",
        "    if name in ('knn', 'svm'):\n",
        "        # these were fit on scaled features; create a wrapper to map grid through scaler\n",
        "        def wrapper_predict_proba(grid):\n",
        "            gsc = scaler.transform(grid)\n",
        "            return model.predict_proba(gsc)\n",
        "        # monkeypatch predict_proba for visualization\n",
        "        class FakeModel:\n",
        "            def predict_proba(self, grid):\n",
        "                return wrapper_predict_proba(grid)\n",
        "        fake = FakeModel()\n",
        "        plot_decision_boundary(fake, X, y, ax=ax, title=name.upper())\n",
        "    else:\n",
        "        plot_decision_boundary(model, X, y, ax=ax, title=name.upper())\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_IV4F2_y69OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One more **helper function**: I’ve merged everything into a single function so you can easily explore how different dataset properties impact model behavior.”"
      ],
      "metadata": {
        "id": "EhSG-TWTimjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_experiment(n_samples=2000, weights=(0.95, 0.05),\n",
        "                    class_sep=1.0, flip_y=0.01, random_state=0):\n",
        "    \"\"\"\n",
        "    Generate synthetic imbalanced dataset, split train/test,\n",
        "    scale features where needed, train four classifiers.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_train, X_test, y_train, y_test : arrays\n",
        "    X_train_scaled, X_test_scaled : arrays (for KNN/SVM)\n",
        "    fitted : dict of trained models\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate data\n",
        "    X, y = generate_data(n_samples=n_samples,\n",
        "                         weights=weights,\n",
        "                         class_sep=class_sep,\n",
        "                         flip_y=flip_y,\n",
        "                         random_state=random_state)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.scatter(X[:,0], X[:,1], c=y, s=15,\n",
        "                cmap='coolwarm', edgecolors='k')\n",
        "    plt.title(f\"Dataset: weights={weights}, sep={class_sep}, noise={flip_y}\")\n",
        "    plt.xlabel('x1'); plt.ylabel('x2')\n",
        "    plt.show()\n",
        "\n",
        "    # Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, stratify=y, random_state=1\n",
        "    )\n",
        "\n",
        "    # Scale for distance-based models\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Define models\n",
        "    models = {\n",
        "        'knn': KNeighborsClassifier(n_neighbors=5),\n",
        "        'tree': DecisionTreeClassifier(max_depth=5, random_state=1),\n",
        "        'svm': SVC(probability=True, gamma='scale', C=1.0, random_state=1),\n",
        "        'logreg': LogisticRegression(solver='lbfgs', max_iter=1000, random_state=1)\n",
        "    }\n",
        "\n",
        "    # Fit\n",
        "    fitted = {}\n",
        "    for name, model in models.items():\n",
        "        if name in ('knn', 'svm'):\n",
        "            model.fit(X_train_scaled, y_train)\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "        fitted[name] = model\n",
        "\n",
        "    return (X_train, X_test, y_train, y_test,\n",
        "            X_train_scaled, X_test_scaled, fitted)\n"
      ],
      "metadata": {
        "id": "wwHSoRI0iuSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Exercises & experiments\n"
      ],
      "metadata": {
        "id": "fffxqSGp3WUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Threshold Effects – Interpretation Exercise\n",
        "\n",
        "We have provided a code snippet that computes predicted probabilities for each classifier, sweeps thresholds from 0 to 1, calculates metrics (accuracy, precision, recall, F1), and plots them versus threshold.\n",
        "\n",
        "**Your task:**\n",
        "\n",
        "1. **Inspect the plots** generated for each classifier.\n",
        "\n",
        "2. **Answer these exploratory questions:**\n",
        "\n",
        "   * How do the classifiers **differ in their response** to changes in the threshold?\n",
        "   * Which classifiers appear **more sensitive** or **less sensitive** to threshold changes?\n",
        "   * For each metric (accuracy, precision, recall, F1), are there classifiers that consistently perform better or worse? Why might this happen?\n",
        "   * Are there thresholds where the “ranking” of classifiers changes depending on the metric?\n",
        "\n",
        "3. **Optional exploration:**\n",
        "\n",
        "   * Adjust the threshold range (e.g., 0.2–0.8) or the step size and observe whether patterns change.\n",
        "   * Consider very low or very high thresholds — what happens to false positives vs false negatives?\n",
        "   * Reflect on how these observations might inform **business or real-world decisions** where the cost of errors differs.\n",
        "\n",
        "> **Hint:** Focus on understanding **how classifier behavior changes with the threshold** rather than just which one is “best.” Think about patterns, sensitivity, and metric trade-offs."
      ],
      "metadata": {
        "id": "h8bBsThZ3bUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "\n",
        "results = {}\n",
        "for name, model in fitted.items():\n",
        "    if name in ('knn', 'svm'):\n",
        "        # use scaled test data\n",
        "        proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    else:\n",
        "        proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    thresholds = np.linspace(0.0, 1.0, 101)\n",
        "    metrics_vs_t = defaultdict(list)\n",
        "    for t in thresholds:\n",
        "        preds = (proba >= t).astype(int)\n",
        "        metrics_vs_t['threshold'].append(t)\n",
        "        metrics_vs_t['accuracy'].append(accuracy_score(y_test, preds))\n",
        "        # guard division by zero in precision/recall\n",
        "        metrics_vs_t['precision'].append(precision_score(y_test, preds, zero_division=0))\n",
        "        metrics_vs_t['recall'].append(recall_score(y_test, preds, zero_division=0))\n",
        "        metrics_vs_t['f1'].append(f1_score(y_test, preds, zero_division=0))\n",
        "\n",
        "    results[name] = pd.DataFrame(metrics_vs_t)\n",
        "\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "\n",
        "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "fig, axes = plt.subplots(4, 1, figsize=(14, 16))\n",
        "\n",
        "# Distinct colors and line styles\n",
        "colors = cm.tab10.colors\n",
        "line_styles = ['-', '--', '-.', ':']\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    ax = axes[i]\n",
        "    for j, (name, df) in enumerate(results.items()):\n",
        "        ax.plot(\n",
        "            df['threshold'],\n",
        "            df[metric],\n",
        "            label=name.upper(),\n",
        "            color=colors[j % len(colors)],\n",
        "            linestyle=line_styles[j % len(line_styles)],\n",
        "            linewidth=2\n",
        "        )\n",
        "    ax.set_title(f'{metric.capitalize()} vs Threshold', fontsize=14)  # panel title\n",
        "    ax.set_ylabel(metric.capitalize())\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_xlabel('Threshold')\n",
        "    ax.grid(True)\n",
        "    ax.legend(fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "jKQtqPAn3dwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question\n",
        "\n",
        "When you sweep the threshold from 0 to 1, why do **KNN and Decision Tree** maintain non-zero precision and recall even at very high thresholds, while **SVM and Logistic Regression** see these metrics drop to near zero?\n",
        "\n",
        "**Hint:** Think about how each model produces probability estimates — are they smooth and continuous, or discrete/step-like?"
      ],
      "metadata": {
        "id": "0TeT-_QGoIlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A2. Threshold Effects – Classifier-Centric View\n",
        "\n",
        "Here, each panel shows a single classifier with **all metrics plotted** as the threshold changes.\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "* See how different metrics behave **for the same classifier**.\n",
        "* Observe **trade-offs** and **sensitivity to threshold changes** within a classifier.\n",
        "* Compare patterns between classifiers to understand **linear vs non-linear behavior**.\n",
        "\n",
        "**Your task:**\n",
        "\n",
        "* Examine the plots for each classifier and **compare across teams**.\n",
        "* **Broad question:** How does each classifier respond to threshold changes across metrics, and what differences do you notice between classifiers?\n",
        "\n",
        "> Focus on overall patterns, trade-offs, and differences rather than exact values."
      ],
      "metadata": {
        "id": "2qewt16XA3Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot metrics vs threshold for each model\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12,10))\n",
        "for ax, (name, df) in zip(axes.ravel(), results.items()):\n",
        "    ax.plot(df['threshold'], df['accuracy'], label='accuracy')\n",
        "    ax.plot(df['threshold'], df['precision'], label='precision')\n",
        "    ax.plot(df['threshold'], df['recall'], label='recall')\n",
        "    ax.plot(df['threshold'], df['f1'], label='f1')\n",
        "    ax.set_title(f'{name.upper()} — metrics vs threshold')\n",
        "    ax.set_xlabel('threshold')\n",
        "    ax.set_xlim(0,1)\n",
        "    ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TL9t2pqxA35k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the dataset properties and how it impacts the decision boundary.\n",
        "\n",
        "Try a harder dataset and see how **decision boundaries** change."
      ],
      "metadata": {
        "id": "2s6rEBjxYegD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_sep= 0.7    # less separation 1 is easy, o → more overlap\n",
        "flip_y=0.05     #  noisy labels\n",
        "weights=(0.95, 0.05)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, fitted = make_experiment(class_sep=class_sep, flip_y=flip_y, weights=weights)\n"
      ],
      "metadata": {
        "id": "Gv57WZE9Yna8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look into the code below, what is the selection rationale?"
      ],
      "metadata": {
        "id": "WJgB125xaO0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find borderline cases using logistic regression\n",
        "probs_lr = fitted['logreg'].predict_proba(X_test)[:, 1]\n",
        "\n",
        "# indices where model is uncertain (probability ~0.5)\n",
        "borderline_idx = np.argsort(np.abs(probs_lr - 0.5))[:3]\n",
        "\n",
        "X_sample = X_test_scaled[borderline_idx]\n",
        "y_sample = y_test[borderline_idx]\n",
        "\n",
        "print(\"Selected sample indices (borderline cases):\", borderline_idx.tolist())\n"
      ],
      "metadata": {
        "id": "wXdVzNpbalbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now print the probabilities for these data samples, how does it look like?"
      ],
      "metadata": {
        "id": "QU4uWcvsaqCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model in fitted.items():\n",
        "    if name in ('knn', 'svm'):\n",
        "        probs = model.predict_proba(X_sample)[:, 1]\n",
        "    else:\n",
        "        probs = model.predict_proba(X_test[borderline_idx])[:, 1]\n",
        "\n",
        "    print(f\"\\n{name.upper()} predictions (borderline samples):\")\n",
        "    for i, (true_label, prob) in enumerate(zip(y_sample, probs)):\n",
        "        pred_default = int(prob >= 0.5)\n",
        "        print(f\"Sample {i}: True={true_label}, Prob={prob:.2f}, Pred@0.5={pred_default}\")\n"
      ],
      "metadata": {
        "id": "M8-s2CwHaykN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now compare how the threshold impact changes when dataset is hard."
      ],
      "metadata": {
        "id": "TV2khGRleRq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "🔑 Key Concept: ROC AUC and Average Precision (PR AUC)\n",
        "\n",
        "So far we looked at metrics like accuracy, precision, recall, F1 at a fixed threshold (usually 0.5).\n",
        "But what if we don’t want to commit to a single threshold yet?\n",
        "\n",
        "That’s where threshold-independent metrics like ROC AUC and PR AUC come in.\n",
        "\n",
        "1. ROC AUC (Area Under the ROC Curve):\n",
        "\n",
        "ROC = Receiver Operating Characteristic curve.\n",
        "\n",
        "Plots True Positive Rate (Recall) vs False Positive Rate (FPR) for all possible thresholds.\n",
        "\n",
        "AUC (area under the curve): the probability that a randomly chosen positive is ranked higher than a randomly chosen negative.\n",
        "\n",
        "Values:\n",
        "\n",
        "0.5 = random guessing\n",
        "\n",
        "1.0 = perfect ranking\n",
        "\n",
        "👉 Good for balanced datasets, but can be overly optimistic when classes are very imbalanced.\n",
        "\n",
        "2. PR AUC (Area under the Precision–Recall curve):\n",
        "\n",
        "Plots Precision vs Recall across thresholds.\n",
        "\n",
        "Average Precision (AP) is the area under this curve.\n",
        "\n",
        "Much more informative than ROC AUC when the positive class is rare.\n",
        "\n",
        "Values:\n",
        "\n",
        "Close to the positive class prevalence = bad\n",
        "\n",
        "Closer to 1.0 = good\n",
        "\n",
        "👉 PR AUC focuses on how well the model finds the minority class without too many false alarms.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FTihTcf33ndj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# ROC curve\n",
        "plt.subplot(1,2,1)\n",
        "for name, model in fitted.items():\n",
        "    if name in ('knn','svm'):\n",
        "        probs = model.predict_proba(X_test_scaled)[:,1]\n",
        "    else:\n",
        "        probs = model.predict_proba(X_test)[:,1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'{name.upper()} (AUC={roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0,1],[0,1],'--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "\n",
        "# PR curve\n",
        "plt.subplot(1,2,2)\n",
        "for name, model in fitted.items():\n",
        "    if name in ('knn','svm'):\n",
        "        probs = model.predict_proba(X_test_scaled)[:,1]\n",
        "    else:\n",
        "        probs = model.predict_proba(X_test)[:,1]\n",
        "    precision, recall, _ = precision_recall_curve(y_test, probs)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    pr_auc=  average_precision_score(y_test, probs)\n",
        "    plt.plot(recall, precision, lw=2, label=f'{name.upper()} (AP={pr_auc:.2f})')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7-ZDIPId3o04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that metrics maybe inconsistent! You are lucky if all metrics tell you the same!"
      ],
      "metadata": {
        "id": "iApOFRXwhqSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Small-sample effects\n",
        "Fit models with small training sets (n=50, 100, 300, 1000) and plot variability.\n",
        "\n",
        "**Exercise C1:** How does metric variance change as sample size increases? Which metric is unstable?\n",
        "\n"
      ],
      "metadata": {
        "id": "slNNSASC3heV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "def small_sample_variability_decision_tree(sample_sizes=[50,100,300,1000], repeats=50, random_state=0):\n",
        "    out = []\n",
        "    for n in sample_sizes:\n",
        "        sss = StratifiedShuffleSplit(n_splits=repeats, train_size=n, random_state=random_state)\n",
        "        for i, (sample_idx, _) in enumerate(sss.split(X, y)):\n",
        "            Xs, ys = X[sample_idx], y[sample_idx]\n",
        "            Xtr, Xv, ytr, yv = train_test_split(\n",
        "                Xs, ys, test_size=0.3, stratify=ys, random_state=i\n",
        "            )\n",
        "\n",
        "            clf = DecisionTreeClassifier(max_depth=3, random_state=random_state)\n",
        "            clf.fit(Xtr, ytr)\n",
        "            preds = clf.predict(X_test)\n",
        "\n",
        "            out.append({\n",
        "                'n': n,\n",
        "                'repeat': i,\n",
        "                'acc': accuracy_score(y_test, preds),\n",
        "                'f1': f1_score(y_test, preds, zero_division=0),\n",
        "                'recall': recall_score(y_test, preds, zero_division=0)\n",
        "            })\n",
        "    return pd.DataFrame(out)\n",
        "\n",
        "\n",
        "# Run experiment\n",
        "var_df = small_sample_variability_decision_tree(sample_sizes=[50,100,300,1000], repeats=50, random_state=42)\n",
        "\n",
        "# Boxplots\n",
        "plt.figure(figsize=(12,4))\n",
        "for i, metric in enumerate([\"acc\", \"f1\", \"recall\"], 1):\n",
        "    plt.subplot(1,3,i)\n",
        "    sns.boxplot(x=\"n\", y=metric, data=var_df)\n",
        "    plt.title(f\"{metric.upper()} across sample sizes\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "u9wjGTp73jUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Study the impact of model difficulty on sample size requirements."
      ],
      "metadata": {
        "id": "4zg7LRVCf06x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v9Qna6pArIPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Exploration tasks for students\n",
        "- Change imbalance ratio in `generate_data()`.\n",
        "- Change `class_sep` to make the task harder/easier.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fZ93zdzZ3ufO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 6. Reference: scikit-learn functions to explore\n",
        "- **Data & Splitting:** `make_classification`, `train_test_split`  \n",
        "- **Models:** `KNeighborsClassifier`, `DecisionTreeClassifier`, `SVC`, `LogisticRegression`  \n",
        "- **Metrics:** `confusion_matrix`, `classification_report`, `precision_score`, `recall_score`,\n",
        "  `f1_score`, `accuracy_score`, `roc_curve`, `roc_auc_score`, `precision_recall_curve`,\n",
        "  `average_precision_score`, `calibration_curve`  \n",
        "- **Utilities:** `StandardScaler`, `Pipeline`, `StratifiedKFold`, `cross_val_score`, `CalibratedClassifierCV`  \n",
        "\n"
      ],
      "metadata": {
        "id": "L_5P9iNU3zNW"
      }
    }
  ]
}